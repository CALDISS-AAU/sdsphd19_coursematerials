{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CALDISS_PHD_Intro_UML.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CALDISS-AAU/sdsphd19_coursematerials/blob/master/notebooks/CALDISS_PHD_Intro_UML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJQTISVhSIPF",
        "colab_type": "text"
      },
      "source": [
        "# Premable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewubjVryR-_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -qq -U matplotlib\n",
        "import pandas as pd #Pandas handles tabular data\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x) # turn off scientific notation and too much decimal blah\n",
        "import matplotlib.pyplot as plt # standard plotting library\n",
        "import numpy as np #Numpy for linear algebra & co\n",
        "import seaborn as sns # For pretty dataviz\n",
        "sns.set_style(\"darkgrid\") # Define style for dataviz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpK2NYvT6Dqu",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fn4douZ7uzH",
        "colab_type": "text"
      },
      "source": [
        "## General\n",
        "\n",
        "As with any concept, machine learning may have a slightly different definition, depending on whom you ask. A little compilation of definitions by academics and practioneers alike:\n",
        "\n",
        "* \"Machine Learning at its most basic is the practice of using algorithms to parse data, learn from it, and then make a determination or prediction about something in the world.\" - Nvidia \n",
        "* \"Machine learning is the science of getting computers to act without being explicitly programmed.\" - Stanford\n",
        "* \"Machine learning is based on algorithms that can learn from data without relying on rules-based programming.\"- McKinsey & Co.\n",
        "* \"Machine learning algorithms can figure out how to perform important tasks by generalizing from examples.\" - University of Washington\n",
        "* \"The field of Machine Learning seeks to answer the question \"How can we build computer systems that automatically improve with experience, and what are the fundamental laws that govern all learning processes?\" - Carnegie Mellon University\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uugFQXKlbz8",
        "colab_type": "text"
      },
      "source": [
        "![](https://www.dropbox.com/s/6mlagf6hfqqw105/random_ai_mask.jpg?dl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzhexkzV9fbZ",
        "colab_type": "text"
      },
      "source": [
        "## Supervised vs. Unsupervised ML\n",
        "\n",
        "![](https://www.dropbox.com/s/45m8ef7qsmqbhvs/super_unsuper2.png?dl=1)\n",
        "\n",
        "![](https://www.dropbox.com/s/yeb7tnyo7vkij42/ML_super_unsuper.png?dl=1)\n",
        "\n",
        "### Supervised ML\n",
        "\n",
        "* Concerned with labeling/classification/input-output-mapping/prediction tasks\n",
        "* Subject of the next lecture, so stay patient\n",
        "\n",
        "### Unsupervised ML\n",
        "\n",
        "Tasks related to pattern recognition and data exploration, in dase there yet does not exist a right answer or problem structure. Main applications:\n",
        "\n",
        "1. **Dimensionality reduction:** Finding patterns in the features of the data\n",
        "2. **Clustering:** Finding homogenous subgroups within larger group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRUD-kgd9zyY",
        "colab_type": "text"
      },
      "source": [
        "# Dimensionality Reduction Techniques\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3CkZmlK94FO",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Dimensionality reduction techniques are foremost useful to (you might see it coming) reduce the dimensionality of our data. So, what does that mean? And why should we want to do that?\n",
        "\n",
        "Dimensions here is a synonym for variables, so what we want to really do is have less variables. To do that, we have to find ways to express the same amount of information with fewer, but more information-rich variables. This is particularly useful to:\n",
        "\n",
        "* Find patterns in the **features** of the data.\n",
        "* Visualization of **high-dimensional** data.\n",
        "* **Pre-processing** before supervised ML tasks (complexity & noise reduction).\n",
        "\n",
        "![](https://www.dropbox.com/s/mtkqlczhp3qusjn/UML_dimensionality_red_types.png?dl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi2ZUglX58Qi",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "# Principal Component Analysis (PCA)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETcque4UQi0A",
        "colab_type": "text"
      },
      "source": [
        "## General\n",
        "\n",
        "* A popular method is \"Principal Component Analysis\" (PCA)\n",
        "* Three goals when finding lower dimensional\n",
        "representation of features:\n",
        "     1. Find linear combination of variables to createprincipal components\n",
        "     2. Maintain most variance in the data\n",
        "     3. Principal components are uncorrelated (i.e.orthogonal to each other)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo5CQ-ZUQjD9",
        "colab_type": "text"
      },
      "source": [
        "## The intuition behind it\n",
        "I won't go into too much detail with the math, but the basics of PCA are as follows: you take a dataset with many variables, and you simplify that dataset by turning your original variables into a smaller number of \"Principal Components\".\n",
        "\n",
        "![](https://www.dropbox.com/s/n8mefi1s2wt9h9g/PCA1.png?dl=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSl4rhe0Z5Kl",
        "colab_type": "text"
      },
      "source": [
        "But what are these exactly? Principal Components are the underlying structure in the data. They are the directions where there is the most variance, the directions where the data is most spread out. This means that we try to find the straight line that best spreads the data out when it is projected along it. This is the first principal component, the straight line that shows the most substantial variance in the data.\n",
        "\n",
        "![](https://www.dropbox.com/s/3iw8vizfick75lh/PCA2.png?dl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kahOjQEoZ8Oi",
        "colab_type": "text"
      },
      "source": [
        "Where many variables correlate with one another, they will all contribute strongly to the same principal component. Each principal component sums up a certain percentage of the total variation in the dataset. Where your initial variables are strongly correlated with one another, you will be able to approximate most of the complexity in your dataset with just a few principal components. Usually, the first principal component captures the main similarity in your data, the second the main difference.\n",
        "\n",
        "![](https://www.dropbox.com/s/tjooyreiguak7oy/PCA3.png?dl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3jOJrqzaBg4",
        "colab_type": "text"
      },
      "source": [
        "These principal components can be computed via **Eigenvalues** and **Eigenvectors**. Just like many things in life, eigenvectors, and eigenvalues come in pairs: every eigenvector has a corresponding eigenvalue. Simply put, an eigenvector is a direction, such as \"vertical\" or \"45 degrees\", while an eigenvalue is a number telling you how much variance there is in the data in that direction. The eigenvector with the highest eigenvalue is, therefore, the first principal component. The number of eigenvalues and eigenvectors that exits is equal to the number of dimensions the data set has. Consequently, we can reframe a dataset in terms of these eigenvectors and eigenvalues without changing the underlying information. \n",
        "\n",
        "Note that reframing a dataset regarding a set of eigenvalues and eigenvectors does not entail changing the data itself, you're just looking at it from a different angle, which should represent the data better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFlOMUTcQxo6",
        "colab_type": "text"
      },
      "source": [
        "## The math behind Eigendecompositions\n",
        "\n",
        "The eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the \"core\" of a PCA: The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. In other words, the eigenvalues explain the variance of the data along the new feature axes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQejvFwgRn3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a matrix\n",
        "mat = np.array([[1, 2, 4], [3, 4, 4], [3, 6, 7]])\n",
        "\n",
        "mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wdre9AXae0yw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# center columns by subtracting column means\n",
        "mat_cent = mat - np.mean(mat.T, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX2O1u4RT-_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute the covariance matrix\n",
        "cov_mat = np.cov(mat_cent.T)\n",
        "print('Covariance matrix by NumPy\\n%s' %cov_mat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcvYiN7JVbsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Next, we perform an eigendecomposition on the covariance matrix:\n",
        "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
        "\n",
        "print('Eigenvectors \\n%s' %eig_vecs)\n",
        "print('\\nEigenvalues \\n%s' %eig_vals)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2u7fBpdt2bY",
        "colab_type": "text"
      },
      "source": [
        "By the way, according to some popular tech-media, you are by now AI-Astrophysicians ;)\n",
        "\n",
        "![](https://www.dropbox.com/s/1onf9d5mn3kl7xu/random_stitchflix_ai.png?dl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMfHZvvdaT_S",
        "colab_type": "text"
      },
      "source": [
        "## Python Tools\n",
        "\n",
        "Ok doing that by hand is fun, but for sure `Python` has you covered for more convenience. PCA analysis is like most other classic UML and SML methods integrated in the `scikitlearn` library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVwMHugGcFlr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the functions we need\n",
        "from sklearn.decomposition import PCA # PCA from  sklearn, the python ML standard library"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHQ_beOdcK7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize a model\n",
        "model = PCA()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb1PU4NicRde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit the model\n",
        "model.fit(mat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jE0JaedZeRf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Access the Eigenvectors (components)\n",
        "model.components_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoNwbALhgeZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How much variance is explained by the components (Eigenvalue)\n",
        "model.explained_variance_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Wv9GOTjhFeG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transform the original data\n",
        "mat_pca = model.transform(mat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QreOmgLMiB6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mat_pca"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk9vPnfE_a3Z",
        "colab_type": "text"
      },
      "source": [
        "## Example\n",
        "\n",
        "Lets explore this with a bit more of an illustrative example. We will use (sorry) the classical `iris` dataset.\n",
        "\n",
        "The Iris flower data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. The data set consists of 50 samples from each of three species of Iris (Iris Setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\n",
        "\n",
        "![](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png)\n",
        "\n",
        "\n",
        "\n",
        "The dataset contains a set of 150 records under 5 attributes \n",
        "\n",
        "* Petal Length\n",
        "* Petal Width\n",
        "* Sepal Length\n",
        "* Sepal width\n",
        "* Class(Species)\n",
        "\n",
        "This dataset became a typical test case for many statistical techniques in machine learning such as clasification, clustering, and dimensionality reduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLVqD6_6VtZ8",
        "colab_type": "text"
      },
      "source": [
        "### Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQxMeFq3i7Cq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load dataset into Pandas DataFrame\n",
        "data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", \n",
        "                 names=['sepal length','sepal width','petal length','petal width','species'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LD26gNFolEE",
        "colab_type": "text"
      },
      "source": [
        "### Brief exploration\n",
        "\n",
        "In the following, we will do some of the standard inspections one should do with every new dataset to get a feeling for its påroperties."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oY5vjrbcqUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check data types\n",
        "data.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjVTW4FwjKaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the first 5 rows\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5ZNH0crlCSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Descriptive statistics\n",
        "data.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wfncV_flMIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here, one of my very favorite visualizations... save that one.\n",
        "# Sns pairplot provides a matrix with scatters on the triangles, and the distribution on the diagonal\n",
        "sns.pairplot(data) # , hue='species'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzcxNkhplZ38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# And lastly, a correlation plot\n",
        "sns.heatmap(data.corr())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaYEBIkKoq_Z",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMVzy6KetNQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transfer the (categorical) outcome of interest to the index (PCA only works with numerical data)\n",
        "data.set_index('species', inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AItSAEN2iZsQ",
        "colab_type": "text"
      },
      "source": [
        "Since in PCA exercises the features used can be of different scale (eg. meter & kilometer) or dispolay a very different variance, it is common practice in ML workflow to normalize features. By doing so, we prevent features with high variance or scale ranges to dominate.\n",
        "\n",
        "We here apply a standard scaler, where we substract for every feature its mean, and thyen divide by its standard deviation.\n",
        "\n",
        "$$StandardScale() = {\\frac {X-\\mu }{\\sigma }}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nMbOHt-tbwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's standard-scale our data\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZV52DQWtr6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# What are mean and Sd now?\n",
        "pd.DataFrame(data_scaled, columns=data.columns).describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3hWdjmuowWp",
        "colab_type": "text"
      },
      "source": [
        "### Executingh PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYTwiVcCqVRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate a PCA object\n",
        "model = PCA() # Number of components needs to be defined. We just for illustration take about half of the numbers of features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMvoRY26u8TS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fitr the model\n",
        "model.fit(data_scaled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSjJ2ngsuM-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit and transform the data\n",
        "data_reduced = model.transform(data_scaled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIMTDC4nqVwV",
        "colab_type": "text"
      },
      "source": [
        "### Inspecting Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD2CNpI6vCwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_data = pd.DataFrame({'evr': model.explained_variance_ratio_, 'cumsum_evr': np.cumsum(model.explained_variance_ratio_)}).stack()\n",
        "sns.set(rc={'figure.figsize':(15,10)})\n",
        "sns.lineplot(y = plot_data.values, x = plot_data.index.get_level_values(0), hue=plot_data.index.get_level_values(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fat1kiwSY7hW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scatter plot of the first 2 componentws\n",
        "sns.scatterplot(x = data_reduced[:,0], y = data_reduced[:,1])\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTuHGkasZ_DN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Correlation of the components\n",
        "sns.heatmap(pd.DataFrame(data_reduced).corr())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYSw8rtVkC40",
        "colab_type": "text"
      },
      "source": [
        "Lets take a look how these components reorganize the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3YTJsLtw6wE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the mean of the grain samples: mean\n",
        "mean = model.mean_\n",
        "\n",
        "# Get the first principal component: first_pc\n",
        "first_pc = model.components_[0,:]\n",
        "second_pc = model.components_[1,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGJ2ceRO5t-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a scatter plot of the untransformed points\n",
        "plt.scatter(data_scaled[:,0], data_scaled[:,1])\n",
        "\n",
        "# Plot first_pc as an arrow, starting at mean\n",
        "plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n",
        "plt.arrow(mean[0], mean[1], second_pc[0], second_pc[1], color='green', width=0.01)\n",
        "\n",
        "# Keep axes on same scale\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXivq39d7Lmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a scatter plot of the untransformed points\n",
        "plt.scatter(data_scaled[:,2], data_scaled[:,3])\n",
        "\n",
        "# Plot first_pc as an arrow, starting at mean\n",
        "plt.arrow(mean[0], mean[1], first_pc[2], first_pc[3], color='red', width=0.01)\n",
        "plt.arrow(mean[0], mean[1], second_pc[2], second_pc[3], color='green', width=0.01)\n",
        "\n",
        "# Keep axes on same scale\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QmeEjYVkRzH",
        "colab_type": "text"
      },
      "source": [
        "We can also look at the components eigenvectors, which give us the loadings of all features on the components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpmBQnmJv2Cy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pcscores = pd.DataFrame(data_reduced)\n",
        "loadings = pd.DataFrame(model.components_, columns=data.columns)\n",
        "loadings.index = ['PC'+str(i+1) for i in range(len(pcscores.columns))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUtgcQFbwQIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ax = sns.heatmap(loadings.transpose(), center=0, linewidths=0.5, \n",
        "                 cmap=\"bone\", vmin=-1, vmax=1, annot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln3VKjFqi2aN",
        "colab_type": "text"
      },
      "source": [
        "# Case study"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60L3komj0aWQ",
        "colab_type": "text"
      },
      "source": [
        "## Introduction to the case\n",
        "\n",
        "![](https://blog.hubspot.com/hubfs/digital-nomad-1.jpg)\n",
        "\n",
        "Allright, lets load some data. Here, we will draw from some own work, where we explore the life of digital nomads. The paper is not written, but the preliminary work is summarized in [this presentation](https://aaudk-my.sharepoint.com/:b:/g/personal/dsh_id_aau_dk/ESeuvplEytZCuNBhKGmA4U8BOGpfbGIbilqTGdgQLA4a6A?e=UGRnvR). \n",
        "\n",
        "You probably already know the data from [NomadList](https://nomadlist.com/). Here, we look at the 2017 crawl of city data, which compiles the digital nomads ranking of cities according to a couple of dimensions. Lets take a look. Roman's web-crawl data is always a bit messy, so we do some little cosmetics upfront."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7mnuGbn0dxV",
        "colab_type": "text"
      },
      "source": [
        "## Loading data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSpJ0irR0TYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We open the data directly from Github\n",
        "cities = pd.read_csv('https://github.com/SDS-AAU/M1-2019/raw/master/data/nomad_cities.csv', sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gum9OBtk0xYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data structures\n",
        "cities.info"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHqR0NA604Qz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cities.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5sYzB9X09SB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cities.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQJzgyZ99HoR",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXs3ihgE1ars",
        "colab_type": "text"
      },
      "source": [
        "For some reason (data is messy) we have the name of the city as well as longitude/langitude but we don't have the country or region names.\n",
        "Let's try to fix that.\n",
        "\n",
        "One approach could be based on the lon/lat values, as city names may non-unique.\n",
        "\n",
        "For this we need to start by \"looking up\" the country name for each specific coordinate\n",
        "\n",
        "We can use an offline reverse geocoder package for that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CE5Lw7h01jK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load and instantiate the reversegeocoder\n",
        "!pip install rgeocoder\n",
        "from rgeocoder import ReverseGeocoder\n",
        "rg = ReverseGeocoder()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFsIisTD1_MR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# That's how it works\n",
        "location = rg.nearest(56.457, 10.039) #lat, lon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaNbBXa42Gby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's see what we get\n",
        "print(location.name)\n",
        "print(location.admin1)\n",
        "print(location.admin2)\n",
        "print(location.cc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOn70i672pUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A smart way to get all geocoding done in one line\n",
        "cities['countrycode'] = cities.apply(lambda t: rg.nearest(t['latitude'],t['longitude']).cc, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saha6Jv8B1SI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cities.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDlRbZkC25bN",
        "colab_type": "text"
      },
      "source": [
        "in the above cell we apply the \n",
        "\n",
        "\n",
        "```\n",
        "rg.nearest(t['latitude'],t['longitude']).cc\n",
        "```\n",
        "\n",
        "function to each row t of the dataframe cities\n",
        "\n",
        "\n",
        "```\n",
        "lambda t:\n",
        "```\n",
        "\n",
        "is a rather scary syntactic way of calling an anonymous function\n",
        "\n",
        "Basically, we could say:\n",
        "\n",
        "\n",
        "*   For each row of the cities dataframe\n",
        "*   take the value of the longitude and latitude columns\n",
        "*   look up the corresponding place\n",
        "*   write the country code \"cc\" into a new column \"alpha-2\"\n",
        "\n",
        "Now that we have exact country codes for each city, we need to find the regions these countries belong to\n",
        "\n",
        "Luckily it's easy to find tables listing this information on the internet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1-3Uvae3Dp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download a recent country info table\n",
        "c = pd.read_csv('https://github.com/SDS-AAU/M1-2019/raw/master/data/countrylist.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3C5hN7x3Hqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfyQLZgU3VIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # Merge this lookup table with our initial cities list\n",
        "cities = cities.merge(c, left_on='countrycode', right_on='alpha_2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZfUGTJ234pf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's select interesting variables for the analysis\n",
        "vars_analysis = [\"cost_nomad\", \"cost_coworking\", \"cost_expat\", \"coffee_in_cafe\", \"cost_beer\", # costs\n",
        "          \"places_to_work\", \"free_wifi_available\", \"internet_speed\", # work\n",
        "          \"freedom_score\", \"peace_score\", \"safety\", \"fragile_states_index\", \"press_freedom_index\", # safety & freedom\n",
        "          \"female_friendly\", \"lgbt_friendly\", \"friendly_to_foreigners\", \"racism\", # friendly\n",
        "          \"leisure\",\"life_score\",\"nightlife\",\"weed\"] # fun \n",
        "\n",
        "vars_descr = [\"nomad_score\", \"cost_nomad\", \"places_to_work\", \"freedom_score\", \"friendly_to_foreigners\", \"life_score\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szGMR2E04KJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# And use the selection to only extract these variables into a new object\n",
        "data = cities[vars_analysis]\n",
        "descr = cities[vars_descr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Plvdkoe14iqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Quick check\n",
        "data.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5v5gXaM4rXn",
        "colab_type": "text"
      },
      "source": [
        "We can see some strange things going with the data types. Some are floats or ints as we would expect but others are objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70m5aR6Z4x3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(data['peace_score'].unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g9sAPoF43qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(data['freedom_score'].unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D73nlIQ461W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(data['fragile_states_index'].unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8s48c6sl49M8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(data['press_freedom_index'].unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzTaFiy-5EtW",
        "colab_type": "text"
      },
      "source": [
        "This is a problem that probably has something to do with the scraping process. Let's try to turn everything into floats and if we can't we just put in a missing value statement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUOH0SQN5FXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function will fo exactly that\n",
        "def floater(x):\n",
        "  try: #Try to\n",
        "    return float(x) #Turn X into a floating point number\n",
        "  except ValueError: #In case a ValueError occurs\n",
        "    return np.nan #Turn X into np.nan (missing value placeholder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8FvwKyW8WzS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# map applies the defined function to every observation\n",
        "data.loc[:,'peace_score'] = data['peace_score'].map(floater)\n",
        "data.loc[:,'freedom_score'] = data['freedom_score'].map(floater)\n",
        "data.loc[:,'fragile_states_index'] = data['fragile_states_index'].map(floater)\n",
        "data.loc[:,'press_freedom_index'] = data['press_freedom_index'].map(floater)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkHI3yYz8iS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# yup, some missing data...\n",
        "data.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2yaohIL8y0O",
        "colab_type": "text"
      },
      "source": [
        "Now we are facing a new problem - Missing Data. While it would be easier to just kick out these cities, we will try to do better and impute.\n",
        "\n",
        "Read up on imputation in the documentation of the package: https://github.com/iskandr/fancyimpute\n",
        "also here: https://medium.com/ibm-data-science-experience/missing-data-conundrum-exploration-and-imputation-techniques-9f40abe0fd87\n",
        "\n",
        "There are many other packages and modules that perform imputation. I found facyimpute the least problematic so far"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJWjMSTQ8z2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the imputation package\n",
        "from fancyimpute import SoftImpute, SimpleFill"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YxrLMm489k7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Easy version: Just replace missing values by the mean of the column\n",
        "data_imp = SimpleFill(fill_method='mean').fit_transform(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc2CWKQf8_QJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can have a quick look\n",
        "pd.DataFrame(data_imp, columns=data.columns).head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4_gfsuB9U2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Or, we go crazy and use a neural network powered method\n",
        "data_imp = SoftImpute().fit_transform(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LwVioUE9cl_",
        "colab_type": "text"
      },
      "source": [
        "Ok, by now our data is pretty much in the shape we want it to be. No missing values, all features are numeric.\n",
        "\n",
        "The last thing to do is to standardize the values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp85Q-oa-Gyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's standard-scale our data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "data_scaled = scaler.fit_transform(data_imp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF_F2aD3-lxQ",
        "colab_type": "text"
      },
      "source": [
        "As you can see, now our data has a mean of 0 and a standard deviation of one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eiwonpL-mXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame(data_scaled, columns=data.columns).describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBCvH6OylLFP",
        "colab_type": "text"
      },
      "source": [
        "## Executing the PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td-OVgEL_d3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the module and instantiate a PCA object\n",
        "from sklearn.decomposition import PCA\n",
        "model = PCA(n_components=7) #We pick 7 as the number of components...just because (it's a 3rd of the columns available)\n",
        "\n",
        "# Fit and transform the data\n",
        "data_reduced = model.fit_transform(data_scaled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ixg9dxpA_jwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make sure the data shape is as it should be\n",
        "data_reduced.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws8mS3kP_rTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_data = pd.DataFrame({'evr': model.explained_variance_ratio_, 'cumsum_evr': np.cumsum(model.explained_variance_ratio_)}).stack()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIU1w47N_tw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Is 7 components really a good choice?\n",
        "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
        "sns.lineplot(y = plot_data.values, x = plot_data.index.get_level_values(0), hue=plot_data.index.get_level_values(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLwGOr6F_2T4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How mach \"information\" do we kick out?\n",
        "model.explained_variance_ratio_.sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QwIa9zAo8_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pcscores = pd.DataFrame(data_reduced)\n",
        "loadings = pd.DataFrame(model.components_, columns=data.columns)\n",
        "loadings.index = ['PC'+str(i+1) for i in range(len(pcscores.columns))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByVCZSmmprEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ax = sns.heatmap(loadings.transpose(), center=0, linewidths=0.5, \n",
        "                 cmap=\"bone\", vmin=-1, vmax=1, annot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw5kvgG9AIYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we can plot in our points\n",
        "plt.figure(figsize=(12,12))\n",
        "g = sns.scatterplot(data_reduced[:,0], data_reduced[:,1],\n",
        "                    legend='full', palette='viridis')\n",
        "\n",
        "legend = g.get_legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JKwEhKyAUWc",
        "colab_type": "text"
      },
      "source": [
        "We can certainly make things much more fancy. However, as you can see, that will require more work.\n",
        "\n",
        "We can use `Bokeh` for an interactive visualisation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRafiaJLHisG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import altair as alt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4nPJ2S2A5a5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the data that we are going to use as a dataframe\n",
        "d = pd.DataFrame({'y': data_reduced[:,1],\n",
        "                 'x': data_reduced[:,0], \n",
        "                 'place': cities.place, \n",
        "                 'country': cities['alpha_2'],\n",
        "                 'region': cities['region']})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STz3jeSuDPF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alt.Chart(d).mark_point().encode(\n",
        "    x='x',\n",
        "    y='y',\n",
        "    color='region',\n",
        "    tooltip=['place', 'country', 'region']\n",
        ").interactive()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehu_c_9_meD5",
        "colab_type": "text"
      },
      "source": [
        "Lastly, lets summarize the characteristics of the data by the different coordinates of component 1 and 2, in 4 categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP-XBmR-3oPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['quadrant'] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVuatzx33Q3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q1 = data[(d['x'] < 0) & (d['y'] < 0)].index\n",
        "q2 = data[(d['x'] < 0) & (d['y'] >= 0)].index\n",
        "q3 = data[(d['x'] >= 0) & (d['y'] < 0)].index\n",
        "q4 = data[(d['x'] >= 0) & (d['y'] >= 0)].index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99vRxgG23jKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['quadrant'][q1] = 1\n",
        "data['quadrant'][q2] = 2\n",
        "data['quadrant'][q3] = 3\n",
        "data['quadrant'][q4] = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU5n0u7a4ZL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.groupby(['quadrant']).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}