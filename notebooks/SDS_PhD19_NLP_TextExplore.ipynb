{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SDS-PhD19 - NLP TextExplore",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CALDISS-AAU/sdsphd19_coursematerials/blob/master/notebooks/SDS_PhD19_NLP_TextExplore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUNRFkoPSYzv",
        "colab_type": "text"
      },
      "source": [
        "## Simple String Manipulation - Freshup :-)\n",
        "\n",
        "We start by taking a piece of text and turning it into something that carries the meaning of the initial text but is less noisy and thus perhaps easier to \"understand\" by a computer\n",
        "\n",
        "![alt text](https://i.guim.co.uk/img/media/ba5ca6316885c50ef827c9fd4f04d7f162a864dc/0_167_5000_3000/master/5000.jpg?width=1140&quality=45&auto=format&fit=max&dpr=2&s=e944b93b711d5a00dc503dd30bf8d60b)\n",
        "\n",
        "Source (image and below text): https://www.theguardian.com/culture/2019/oct/09/muslim-drag-queen-amrou-al-kadhi-whenever-the-drag-came-off-id-have-a-nervous-breakdown"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjS20URioVlh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"The Eton-educated, non-binary British Iraqi had always struggled with their identity, until they discovered drag. Yet the 29 year old says the performances come at a high price\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSPpEsXwoVlj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split on fullstop\n",
        "text.lower().split(\".\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8wzPgqAoVlm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split on empty space\n",
        "text.split(\" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8ba_2LOoVlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find in text (position)\n",
        "text.find('29')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P391O7JoVlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simple replacement\n",
        "text.replace('o', 'O')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiNk7YmWoVlt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# very short RegEx\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvidsjTloVlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "re.findall(r'\\d+', text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBB8xJVYoVlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numbers = re.findall(r'\\d+', text)\n",
        "\n",
        "for i in numbers:\n",
        "    print(text.replace(i, str(int(i) + 1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7ImVLtHoVlz",
        "colab_type": "text"
      },
      "source": [
        "More on RegEx in the Datacamp courses (there is a whole course on that actually) and [here](https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZzQsIYGrXM1",
        "colab_type": "text"
      },
      "source": [
        "Overall, in NLP we are trying to represent meaning structure. That means that we want to focus on the most important and \"meaning-bearing elements\" in text, while reducing noise.\n",
        "Words such as \"and\", \"have\", \"the\" may have central syntactic functions but are not particularly important from a semantic perspective.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlZ4knHPoVl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining stopwords\n",
        "\n",
        "stopwords_en = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
        "                'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \n",
        "                \"you'd\", 'your', 'yours', 'yourself', 'yourselves', \n",
        "                'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', \n",
        "                'hers', 'herself', 'it', \"it's\", 'its', 'itself', \n",
        "                'they', 'them', 'their', 'theirs', 'themselves', 'what', \n",
        "                'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', \n",
        "                'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
        "                'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', \n",
        "                'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', \n",
        "                'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
        "                'between', 'into', 'through', 'during', 'before', 'after', 'above', \n",
        "                'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', \n",
        "                'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', \n",
        "                'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', \n",
        "                'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', \n",
        "                'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', \n",
        "                'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', \n",
        "                'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \n",
        "                \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \n",
        "                \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", \n",
        "                'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \n",
        "                'won', \"won't\", 'wouldn', \"wouldn't\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13czGzLmoVl2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's keep only words that are not stopwords\n",
        "[word for word in text.lower().split() if word not in stopwords_en]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln9stkrdoVl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's use RegEx one more time to remove leading or trailing punctuation from our words\n",
        "'drag.,'.strip(r'[\" ,.!?:;\"]')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsyI6t1-oVl7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's combine that and add another condition \"No numbers\"\n",
        "[word.strip(r'[\" ,.!?:;\"]') for word in text.lower().split() if word not in stopwords_en and not word.isdigit()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whVULFm5oVl9",
        "colab_type": "text"
      },
      "source": [
        "Now that you undestand (hopefully) what’s going on on the basic level, let's start using some more sophisticated tools to work with text.\n",
        "\n",
        "We will import some tokenizers from NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZOY9IcapnCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk #this part is needed on colab.\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "#----------------------------------------\n",
        "\n",
        "# Tokenizing sentences\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Tokenizing words\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wbQ7SsXoVmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's get our stences.\n",
        "# Note that the full-stops at the end of each sentence are still there\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUEFiD0qoVmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use word_tokenize to tokenize the third sentence: tokenized_sent\n",
        "tokenized_sent = word_tokenize(sentences[1])\n",
        "\n",
        "# Make a set of unique tokens in the entire scene: unique_tokens\n",
        "unique_tokens = set(word_tokenize(text))\n",
        "\n",
        "print(tokenized_sent)\n",
        "print(unique_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgdJTl_5pZnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english')) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G50Jl_MrpyLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[word.lower() for word in word_tokenize(text) if word not in stop_words and word.isalnum()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6SkRIxl9b4V",
        "colab_type": "text"
      },
      "source": [
        "#### Your turn!\n",
        "\n",
        "![alt text](https://media.giphy.com/media/9rwFfmB2qJ0mEsmkfj/giphy.gif)\n",
        "\n",
        "Take the following text and transform it into a list of lists with with each element being a tokenized sentence. Remove stopwords, lower all tokens and keep only alpha-numeric tokens.\n",
        "\n",
        "\n",
        "\"I’ve been called many things in my life, but never an optimist. That was fine by me. I believed pessimists lived in a constant state of pleasant surprise: if you always expected the worst, things generally turned out better than you imagined. The only real problem with pessimism, I figured, was that too much of it could accidentally turn you into an optimist.\"\n",
        "\n",
        "source: https://www.theguardian.com/global/2019/nov/21/glass-half-full-how-i-learned-to-be-an-optimist-in-a-week\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFEDijtCoVmE",
        "colab_type": "text"
      },
      "source": [
        "### Processing many short texts and simple stats\n",
        "\n",
        "An introduction to NLP would not be the same without Donald's tweets. Let's use these tweets for some more basic NLP and let's try to gather some insights...maybe\n",
        "\n",
        "![donald_tweets](https://i.cdn.cnn.com/cnn/interactive/2017/politics/trump-tweets/media/trump-tweets-hdr-02.jpg)\n",
        "\n",
        "Let's try to use some very simple statistics on twitter data:\n",
        "\n",
        "thanks to [Trump Twitter Archive](http://www.trumptwitterarchive.com)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eC60LPQoVmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', -1) #to see more text\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCNZoQA3oVmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizing Tweets made easy!\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tknzr = TweetTokenizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIcUPTayoVmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download and open some Trump tweets from trump_tweet_data_archive\n",
        "\n",
        "trump_tweets_df = pd.read_json('https://github.com/bpb27/trump_tweet_data_archive/raw/master/condensed_2018.json.zip')\n",
        "trump_tweets_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPWtWgyaoVmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reset index (not really needed but why not)\n",
        "trump_tweets_df = trump_tweets_df.set_index(pd.to_datetime(trump_tweets_df.created_at))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbQtp1mDoVmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testing the tokenizer\n",
        "tknzr.tokenize(\"I am a very #cool tweet by @Roman\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDU9J4PCoVmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's identify people Trump likes to mention\n",
        "trump_tweets_df['mentions'] = trump_tweets_df['text'].map(lambda textline: [tag for tag in tknzr.tokenize(textline) if tag.startswith('@')])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOwtYQ5aoVmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Only keep tweets where a mention i present\n",
        "trump_tweets_df = trump_tweets_df[trump_tweets_df['mentions'].map(len) > 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F36Hl_toVmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Collect\n",
        "trump_tags = itertools.chain(*trump_tweets_df['mentions'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx7l7KjXoVma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Count up and show\n",
        "counted_tags = Counter(trump_tags)\n",
        "counted_tags.most_common()[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbB076gutlF1",
        "colab_type": "text"
      },
      "source": [
        "#### Your turn\n",
        "![alt text](https://media.giphy.com/media/JIX9t2j0ZTN9S/giphy.gif)\n",
        "\n",
        "The link below holds a datasewt with ~10k #OKBoomer tweets from the days 10-21 Nov.\n",
        "\n",
        "https://github.com/CALDISS-AAU/sdsphd19_coursematerials/raw/master/data/tweets_boomer.zip\n",
        "\n",
        "Use elements from the above code to make a list of the most common hashtags (you have to get the hashtags from the text, not using the column containing them already)\n"
      ]
    }
  ]
}