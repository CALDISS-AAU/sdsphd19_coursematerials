{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SDS_PHD_Explainable_ML.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CALDISS-AAU/sdsphd19_coursematerials/blob/master/notebooks/SDS_PHD_Explainable_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P4lpEsa5jZT"
      },
      "source": [
        "# Standard stuff\n",
        "import pandas as pd #for manipulating data\n",
        "import numpy as np #for manipulating data\n",
        "\n",
        "# Dataviz\n",
        "import matplotlib.pyplot as plt #for custom graphs at the end\n",
        "import seaborn as sns #for custom graphs at the end\n",
        "\n",
        "# Other tooling\n",
        "import os #needed to use Environment Variables in Domino\n",
        "\n",
        "# SML\n",
        "import sklearn #for building models\n",
        "import xgboost as xgb #for building models\n",
        "import sklearn.ensemble #for building models\n",
        "from sklearn.model_selection import train_test_split #for creating a hold-out sample\n",
        "from sklearn import datasets # Boston Housing Data\n",
        "\n",
        "# Explainable ML&AI tools\n",
        "!pip install lime\n",
        "import lime #LIME package\n",
        "import lime.lime_tabular #the type of LIIME analysis we’ll do\n",
        "!pip install shap\n",
        "import shap #SHAP package\n",
        "import yellowbrick as yb\n",
        "!pip install pdpbox\n",
        "from pdpbox import pdp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub2qq4s2Tw19"
      },
      "source": [
        "# Introduction to Explainable ML&AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2bY0BQ0UPKe"
      },
      "source": [
        "Machine learning (ML) models are often considered black boxes due to their complex inner-workings. More advanced ML models such as random forests, gradient boosting machines (GBM), artificial neural networks (ANN), among others are typically more accurate for predicting nonlinear, faint, or rare phenomena. Unfortunately, more accuracy often comes at the expense of interpretability, and interpretability is crucial for business adoption, model documentation, regulatory oversight, and human acceptance and trust. \n",
        "\n",
        "![](https://miro.medium.com/max/3036/1*vfphlzE27jHerTIe30ICsw.png)\n",
        "\n",
        "However, for situations where the social/economic costs of failure are high (plane crashes, who gets an insurance or has to go to prison), there is a need for explainability, interpretability, and accountability of algorithmic decisions.\n",
        "\n",
        "![](https://www.dropbox.com/s/dl4xlxwl583cehi/random_computer_says_no.png?dl=1)\n",
        "\n",
        "Luckily, several advancements have been made to aid in interpreting ML models.\n",
        "\n",
        "Broadly, one can classify such aproaches as:\n",
        "\n",
        "1. Global Explanations\n",
        "2. Local Explanations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLA8OOnu8M83"
      },
      "source": [
        "# Prediction Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21uvCV-8VyFg"
      },
      "source": [
        "To illustrate a few features I am going to be using a scikit-learn dataset called the wine recognition set. This dataset has 13 features and 3 target classes and can be loaded directly from the scikit-learn library. In the below code I am importing the dataset and converting it to a data frame. The data can be used in a classifier without any additional preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwhnXYdW8QsI"
      },
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "wine_data = datasets.load_wine()\n",
        "df_wine = pd.DataFrame(wine_data.data,columns=wine_data.feature_names)\n",
        "df_wine['target'] = pd.Series(wine_data.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC9yzbbRV6kp"
      },
      "source": [
        "df_wine.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bidw1cW_827H"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_wine.drop(['target'], axis=1)\n",
        "y = df_wine['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0ouKLJPiztT"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WHjVUgZWKFv"
      },
      "source": [
        "# Global Explanations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "009kHX-cZfAX"
      },
      "source": [
        "## Yellowbricks\n",
        "\n",
        "This library is essentially an extension of the scikit-learn library and provides some really useful and pretty looking visualisations for machine learning models. The visualiser objects, the core interface, are scikit-learn estimators and so if you are used to working with scikit-learn the workflow should be quite familiar.\n",
        "\n",
        "The visualisations that can be rendered cover model selection, feature importances and model performance analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qTuUErW9Mvn"
      },
      "source": [
        "from yellowbrick.classifier import ClassificationReport\n",
        "\n",
        "visualizer = ClassificationReport(model, size=(1080, 720))\n",
        "visualizer.fit(X_train, y_train)\n",
        "visualizer.score(X_test, y_test)\n",
        "visualizer.poof()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRWkGqKYjOtp"
      },
      "source": [
        "# Feature importance\n",
        "from yellowbrick.features import FeatureImportances\n",
        "\n",
        "viz = FeatureImportances(model)\n",
        "viz.fit(X, y)\n",
        "viz.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAE3UFJt9V8u"
      },
      "source": [
        "## ELI5\n",
        "\n",
        "ELI5 is another visualisation library that is useful for debugging machine learning models and explaining the predictions they have produced. It works with the most common python machine learning libraries including scikit-learn, XGBoost and Keras.\n",
        "\n",
        "It provides easy functionality for creating model specific measure of globel feature importance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kuEpz279XOV"
      },
      "source": [
        "!pip install eli5\n",
        "import eli5\n",
        "eli5.show_weights(model, feature_names = X.columns.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpAyqSK2ZG68"
      },
      "source": [
        "By default the show_weights method uses gain to calculate the weight but you can specify other types by adding the importance_type argument.\n",
        "\n",
        "You can also use show_prediction to inspect the reasons for individual predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aKjxa6C9bIC"
      },
      "source": [
        "from eli5 import show_prediction\n",
        "show_prediction(model, X_train.iloc[1], feature_names = X.columns.tolist(), \n",
        "                show_feature_values=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R2JCFWy9_Re"
      },
      "source": [
        "## ML extend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McB8i2biX7DV"
      },
      "source": [
        "This library contains a host of helper functions for machine learning. This covers things like stacking and voting classifiers, model evaluation, feature extraction and engineering and plotting. In addition to the documentation, [this paper](https://sebastianraschka.com/pdf/software/mlxtend-latest.pdf) is a good resource for a more detailed understanding of the package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzggG8iC-B9i"
      },
      "source": [
        "!pip install mlxtend"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6qpfuzA-GG-"
      },
      "source": [
        "from mlxtend.plotting import plot_decision_regions\n",
        "from mlxtend.classifier import EnsembleVoteClassifier\n",
        "\n",
        "import matplotlib.gridspec as gridspec\n",
        "import itertools \n",
        "from sklearn import model_selection\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh6tAKL8-bQR"
      },
      "source": [
        "# Unfortunatelly only works for 2 features at once.\n",
        "X_train_ml = X_train[['proline', 'color_intensity']].values\n",
        "y_train_ml = y_train.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k60wEtSz-dnG"
      },
      "source": [
        "# We run a variety of models here\n",
        "clf1 = LogisticRegression(random_state=1)\n",
        "clf2 = RandomForestClassifier(random_state=1)\n",
        "clf3 = GaussianNB()\n",
        "eclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[1,1,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sABecrEYQ8z"
      },
      "source": [
        "# And plot it\n",
        "value=1.5\n",
        "width=0.75\n",
        "gs = gridspec.GridSpec(2,2)\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "labels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'Ensemble']\n",
        "for clf, lab, grd in zip([clf1, clf2, clf3, eclf],\n",
        "                         labels,\n",
        "                         itertools.product([0, 1], repeat=2)):\n",
        "                         \n",
        "    clf.fit(X_train_ml, y_train_ml)\n",
        "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
        "    fig = plot_decision_regions(X=X_train_ml, y=y_train_ml, clf=clf)\n",
        "    plt.title(lab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEAMj4oXWQrT"
      },
      "source": [
        "# Local Explanations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4ak7se09mGW"
      },
      "source": [
        "## Lime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xOCkGNBGk7M"
      },
      "source": [
        "\n",
        "### Introducing [`lime`](https://github.com/marcotcr/lime)\n",
        "\n",
        "> *\"There once was a package called lime, Whose models were simply sublime, It gave explanations for their variations, one observation at a time.\"*\n",
        "\n",
        "*lime-rick by Mara Averick*\n",
        "\n",
        "**Local Interpretable Model-agnostic Explanations** (LIME) is a visualization technique that helps explain individual predictions. As the name implies, it is model agnostic so it can be applied to any supervised regression or classification model. The original paper is mindblowing, if you find time, just read it!\n",
        "\n",
        "* Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"\"hy Should I Trust You?: Explaining the Predictions of Any Classifier.\" In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2016). ACM, New York, NY, USA, 1135-1144. DOI: https://doi.org/10.1145/2939672.2939778\n",
        "\n",
        "Behind the workings of LIME lies the assumption that every complex model is linear on a local scale and asserting that it is possible to fit a simple model around a single observation that will mimic how the global model behaves at that locality. The simple model can then be used to explain the predictions of the more complex model locally.\n",
        "\n",
        "The generalized algorithm LIME applies is:\n",
        "\n",
        "1. Given an observation, permute it to create replicated feature data with slight value modifications.\n",
        "2. Compute similarity distance measure between original observation and permuted observations.\n",
        "3. Apply selected machine learning model to predict outcomes of permuted data.\n",
        "3. Select m number of features to best describe predicted outcomes.\n",
        "4. Fit a simple model to the permuted data, explaining the complex model outcome with m features from the permuted data weighted by its similarity to the original observation .\n",
        "5. Use the resulting feature weights to explain local behavior.\n",
        "\n",
        "A little example on image data (original application)\n",
        "\n",
        "![](https://www.dropbox.com/s/wyimw0dw5b8ifhb/ml_lime_example.png?dl=1)\n",
        "\n",
        "Or one on text...\n",
        "\n",
        "![](https://raw.githubusercontent.com/marcotcr/lime/master/doc/images/twoclass.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv9TGYkFWiTc"
      },
      "source": [
        "### How to apply"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqAl1Q589nZg"
      },
      "source": [
        "import lime.lime_tabular\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values,                                            \n",
        "                 feature_names=X_train.columns.values.tolist(),                                        \n",
        "                 class_names=y_train.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQHABGVB9sAH"
      },
      "source": [
        "predict_fn = lambda x: model.predict_proba(x).astype(float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZexnTMJH1Lo"
      },
      "source": [
        "The `explain` function above first creates permutations, then calculates similarities, followed by selecting the m features. Lastly, explain will then fit a model. `lime` applies a ridge regression model (a subgroup of elastic nets) with the weighted permuted observations as the simple model.If the model is a regressor, the simple model will predict the output of the complex model directly. If the complex model is a classifier, the simple model will predict the probability of the chosen class(es).\n",
        "\n",
        "The `explain` output is a data frame containing different information on the simple model predictions. Most importantly, for each observation  it contains the simple model fit  and the weighted importance (feature_weight) for each important feature that best describes the local relationship."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4eCzLGd9wdQ"
      },
      "source": [
        "exp = explainer.explain_instance(X_test.values[10], predict_fn, num_features=6)\n",
        "exp.show_in_notebook(show_all=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXbrU8NqPTjK"
      },
      "source": [
        "## SHAP Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pKmwmiHQj92"
      },
      "source": [
        "SHAP and LIME are both popular Python libraries for model explainability. SHAP (SHapley Additive exPlanation) leverages the idea of [Shapley values](https://christophm.github.io/interpretable-ml-book/shapley.html) for model feature influence scoring (for details, check [this NEURIPS paper](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf)). The technical definition of a Shapley value is the “average marginal contribution of a feature value over all possible coalitions.” In other words, Shapley values consider all possible predictions for an instance using all possible combinations of inputs. Because of this exhaustive approach, SHAP can guarantee properties like consistency and local accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN3637EaXIFG"
      },
      "source": [
        "### How to apply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCu2KBqLRpC2"
      },
      "source": [
        "We first again create an explainer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4cXHlDvYOga"
      },
      "source": [
        "# Lime takes the model, and does the prediction afterwards. For sharp I already have to fit the model\n",
        "model_rf = sklearn.ensemble.RandomForestRegressor()\n",
        "model_rf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lqoe3maPWUx"
      },
      "source": [
        "# Define the explainer\n",
        "explainer_shap = shap.TreeExplainer(model_rf)\n",
        "shap_values_test = explainer_shap.shap_values(X_test)\n",
        "shap_values_train = explainer_shap.shap_values(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCztQs7GPaoH"
      },
      "source": [
        "# Run\n",
        "df_shap_test = pd.DataFrame(shap_values_test, columns=X_test.columns.values)\n",
        "df_shap_train = pd.DataFrame(shap_values_train, columns=X_train.columns.values)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Idv2gw4PhtK"
      },
      "source": [
        "# if a feature has 10 or less unique values then treat it as categorical\n",
        "categorical_features = np.argwhere(np.array([len(set(X_train.values[:,x]))\n",
        "for x in range(X_train.values.shape[1])]) <= 10).flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzQ9w0LcPqaF"
      },
      "source": [
        "# j will be the record we explain\n",
        "j = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt2RCFWBPxNp"
      },
      "source": [
        "# initialize js for SHAP\n",
        "shap.initjs()\n",
        "shap.force_plot(explainer_shap.expected_value, shap_values_test[j], X_test.iloc[[j]]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXxXXdnXXZ1x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}